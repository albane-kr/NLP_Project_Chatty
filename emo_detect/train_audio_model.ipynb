{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import soundfile as sf\n",
    "torch.device(device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionalSpeechDataset(Dataset):\n",
    "    def __init__(self, labels_files, transform=None):\n",
    "        self.transform = transform\n",
    "        self.file_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Read the labels file\n",
    "        for labels_file in labels_files:\n",
    "            os.listdir(\"c:/Users/henri/Documents/Uni.lu/Semester 5/NLP/Project/NLP_Project/emo_detect/dataset/EmotionSpeechDataset/0001\")\n",
    "            with open(labels_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    file_name = parts[0] + '.wav'\n",
    "                    label = parts[-1]\n",
    "                    self.file_paths.append(os.path.join(f\"c:/Users/henri/Documents/Uni.lu/Semester 5/NLP/Project/NLP_Project/emo_detect/dataset/EmotionSpeechDataset/{file_name[:4]}/{label}\", file_name))\n",
    "                    self.labels.append(label)\n",
    "        \n",
    "        # Map emotions to numerical labels\n",
    "        self.label_map = {label: idx for idx, label in enumerate(set(self.labels))}\n",
    "        self.labels = [self.label_map[label] for label in self.labels]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            waveform = self.transform(waveform)\n",
    "        \n",
    "        return waveform, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionRecognitionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EmotionRecognitionModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(16 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, 5)  # Assuming 5 emotion classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 16 * 32 * 32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(batch):\n",
    "    # Separate the waveforms and labels\n",
    "    waveforms = [item[0].squeeze(0) for item in batch]  # Remove channel dimension if necessary\n",
    "    labels = torch.tensor([item[1] for item in batch])\n",
    "    \n",
    "    # Pad the waveforms\n",
    "    waveforms_padded = torch.nn.utils.rnn.pad_sequence(waveforms, batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Add the channel dimension back\n",
    "    waveforms_padded = waveforms_padded.unsqueeze(1)\n",
    "    \n",
    "    return waveforms_padded, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training on {device}\")\n",
    "model = EmotionRecognitionModel().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 100\n",
    "transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=16, n_fft=512)\n",
    "labels_files = [\"./dataset/EmotionSpeechDataset/0011/0011.txt\",\n",
    "                \"./dataset/EmotionSpeechDataset/0012/0012.txt\",\n",
    "                \"./dataset/EmotionSpeechDataset/0013/0013.txt\",\n",
    "                \"./dataset/EmotionSpeechDataset/0014/0014.txt\",\n",
    "                \"./dataset/EmotionSpeechDataset/0015/0015.txt\",\n",
    "                \"./dataset/EmotionSpeechDataset/0016/0016.txt\",\n",
    "                \"./dataset/EmotionSpeechDataset/0017/0017.txt\",\n",
    "                \"./dataset/EmotionSpeechDataset/0018/0018.txt\",\n",
    "                \"./dataset/EmotionSpeechDataset/0019/0019.txt\",\n",
    "                \"./dataset/EmotionSpeechDataset/0020/0020.txt\",\n",
    "                ]\n",
    "dataset = EmotionalSpeechDataset(labels_files, transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle=True, num_workers=4, collate_fn=pad_sequence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
